{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Machine Learning\n",
    "\n",
    "In this assignment, we will get you going on building simple machine learning models. We will use a popular Python machine learning library called [`scikit-learn`](https://scikit-learn.org/stable/index.html) and work on the `Titanic` dataset, a popular introductory dataset for binary classification that includes information on the passengers of the Titanic. In particular, we will try to build a model to estimate whether a passenger survived or not, based on the rest of the available information.\n",
    "\n",
    "Before we begin, make sure you have chosen the right virtual environment (iap-data-venv) to run this notebook in. How to do this depends on your IDE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the Titanic dataset in `datasets/ml/titanic.csv`. Let's take a look at the schema of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data  = pd.read_csv('../datasets/ml/titanic.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset contains the following fields for each of 891 passengers:\n",
    "\n",
    "- `PassengerId`: A unique, monotonically increasing ID number for each passenger.\n",
    "- `Survived`: Whether or not the passenger survived.\n",
    "- `Pclass`: The class of the passenger (1 = 1st; 2 = 2nd; 3 = 3rd).\n",
    "- `Name`, `Sex`, `Age`: Demographic informaton about the passenger.\n",
    "- `SibSp` - Number of siblings/spouses aboard.\n",
    "- `Parch` - Number of parents/children aboard.\n",
    "- `Fare` - Fare paid by the passenger.\n",
    "- `Cabin` - Cabin number.\n",
    "- `Embarked` - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "Let's preprocess the dataset to drop missing values, encode categorical vairables and create a training and testing split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Drop passengers with missing age\n",
    "data.dropna(subset='Age', inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Splitting data\n",
    "X = data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n",
    "y = data['Survived']\n",
    "\n",
    "# Append columns with one-hot encoding for Sex and Embarked\n",
    "X = pd.concat([X, pd.get_dummies(X['Sex'], prefix='Sex')], axis=1)\n",
    "X = pd.concat([X, pd.get_dummies(X['Embarked'], prefix='Embarked')], axis=1)\n",
    "X.drop(columns=['Sex', 'Embarked'], inplace=True)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have now avoided having categorical columns, notice that the scale of our columns is different: `Pclass` only ranges from 1 to 3, while `Age` can take much larger values. To avoid improperly weighting these features just because their scales are different, it is common to apply a futher preprocessing step where we standardize them. Here we will use `preprocessing.StandardScaler()` to ensure that every numerical column has zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Scale numerical columns\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to experiment with differnt ML classifiers available in `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). Logistic Regression is a linear model for classification. It is very similar to Linear Regression, but instead of predicting a continuous value, it predicts the probability of an instance belonging to a class. We provide the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_predictions = logistic_model.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"Accuracy on training data: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "test_predictions = logistic_model.predict(X_test_scaled)\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Accuracy on test data: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next try a [decision tree classifier](https://scikit-learn.org/stable/modules/tree.html#classification).  Decision trees are a type of model that can be used for both classification and regression.  They are easy to interpret and visualize.  They are also the basis for more complex models such as random forests and gradient boosted trees.\n",
    "\n",
    "\n",
    "Take a look at the documentation and fill in the blaks below. You will notice that t`scikit-learn` provides the same interface regardless of model, so the code will look very similar to what we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize and train model\n",
    "tree_model = ... # TODO: Create a model\n",
    "# TODO: Fit the model to the data\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_predictions = ... # TODO: Derive predictions for training data\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"Accuracy on training data: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "test_predictions = ... # TODO: Derive predictions for test data\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Accuracy on test data: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will experiment with a [random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), an ensemble method that combines multiple decision trees to create a more powerful model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train model\n",
    "forest_model = ... # TODO: Create a model\n",
    "# TODO: Fit the model to the data\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_predictions = ... # TODO: Derive predictions for training data\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"Accuracy on training data: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "test_predictions = ... # TODO: Derive predictions for test data\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Accuracy on test data: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the random forest achieves the same accuracy on the training data but manages to generalize better to the test data. This is because the random forest is less prone to overfitting than a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 4: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will also try a [support vector machine (SVM)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). SVMs are a powerful class of models that can be used for both classification and regression. They work by finding a hyperplane that separates the data into two classes. The hyperplane is chosen such that the distance between the hyperplane and the nearest data point from each class is maximized. This distance is called the margin. The data points that are closest to the hyperplane are called support vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train model\n",
    "svm_model = ... # TODO: Create a model\n",
    "# TODO: Fit the model to the data\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_predictions = ... # TODO: Derive predictions for training data\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"Accuracy on training data: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "test_predictions = ... # TODO: Derive predictions for test data\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Accuracy on test data: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 5: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will try [k-Nearest Neighbors (kNN) classification](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). This algorithm is a bit different from the previous ones, as it does not learn a model. Instead, it memorizes the training data and uses it to classify new data points based on the k nearest points in the training data. The kNN algorithm is an example of an instance-based learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize and train model\n",
    "knn_model = ... # TODO: Create a model\n",
    "# TODO: Fit the model to the data\n",
    "\n",
    "# Predictions and evaluation\n",
    "train_predictions = ... # TODO: Derive predictions for training data\n",
    "train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "print(f\"Accuracy on training data: {train_accuracy*100:.2f}%\")\n",
    "\n",
    "test_predictions = ... # TODO: Derive predictions for test data\n",
    "test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Accuracy on test data: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iap-data-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
